import random
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from rdkit.DataStructs import BulkTanimotoSimilarity, ExplicitBitVect
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, classification_report
from sklearn.cluster import DBSCAN
from Preprocess import Morgan_fingerprints, dataset, Molecular_Descriptors


# Morgan Fingerprint + DBSCAN (for reference and comparison)
class MorganFingerprintClustering:
    def __init__(self, dataset, sample_size=2000):
        self.dataset = dataset
        self.sample_size = sample_size
        self.fingerprints_morgan = None
        self.sample_fps = None
        self.sample_idx = None
        self.distance_matrix = None
        self.sample_labels = None
        self.all_labels = None

    def prepare_data(self):
        print("=" * 70)
        print("METHOD 1: Morgan Fingerprint + DBSCAN")
        print("=" * 70)
        self.fingerprints_morgan = Morgan_fingerprints(self.dataset.canonical_smiles)
        n_total = len(self.fingerprints_morgan)
        self.sample_idx = sorted(random.sample(range(n_total), self.sample_size))
        self.sample_fps = [self.fingerprints_morgan[i] for i in self.sample_idx]
        print(f"Total molecules: {n_total}, sampled: {self.sample_size}")
        return self

    def calculate_tanimoto_distance(self): #generated by AI
        fingerprints = [fp for fp in self.sample_fps if isinstance(fp, ExplicitBitVect)]
        n = len(fingerprints)
        print(f"Valid fingerprints: {n}")
        dist_matrix = np.zeros((n, n), dtype=np.float32)
        for i in range(n):
            sims = BulkTanimotoSimilarity(fingerprints[i], fingerprints)
            dist_matrix[i, :] = [1 - s for s in sims]
            if i % 500 == 0:
                print(f"Processed {i}/{n}")
        self.distance_matrix = dist_matrix
        np.save("tanimoto_distance_sample.npy", dist_matrix)
        print("Saved distance matrix to tanimoto_distance_sample.npy\n")
        return self

    def optimize_dbscan(self, eps_values=[0.35, 0.4, 0.45, 0.5]): # revised by AI
        print("Testing different eps values for DBSCAN:")
        best_eps, best_score = None, -1
        for eps in eps_values:
            db = DBSCAN(eps=eps, min_samples=5, metric="precomputed")
            labels = db.fit_predict(self.distance_matrix)
            unique, counts = np.unique(labels, return_counts=True)
            print(f"eps={eps:.2f} -> {dict(zip(unique, counts))}")
            if len(set(labels)) > 1 and len(set(labels)) < len(self.distance_matrix):
                D = 1 - self.distance_matrix
                np.fill_diagonal(D, 0)
                score = silhouette_score(D, labels, metric="precomputed")
                print(f"    silhouette score = {score:.3f}")
                if score > best_score:
                    best_score, best_eps = score, eps
            else:
                print("    silhouette score = N/A")
        print(f"\nBest eps selected: {best_eps} with silhouette={best_score:.3f}\n")
        dbscan = DBSCAN(eps=best_eps, min_samples=5, metric="precomputed")
        self.sample_labels = dbscan.fit_predict(self.distance_matrix)
        unique, counts = np.unique(self.sample_labels, return_counts=True)
        print("Cluster distribution in sample:")
        for u, c in zip(unique, counts):
            print(f"  Cluster {u}: {c} molecules")
        return self

    def map_to_full_dataset(self):
        print("\nMapping all molecules to nearest sampled cluster...")
        n_total = len(self.fingerprints_morgan)
        self.all_labels = np.full(n_total, -1)
        for i in range(n_total):
            sims = BulkTanimotoSimilarity(self.fingerprints_morgan[i], self.sample_fps)
            if max(sims) > 0.3:
                nearest = np.argmax(sims)
                self.all_labels[i] = self.sample_labels[nearest]
            if i % 2000 == 0:
                print(f"Mapped {i}/{n_total}")
        np.save("cluster_labels_all.npy", self.all_labels)
        print("Saved all molecule cluster labels to cluster_labels_all.npy\n")
        return self

    def run(self):
        self.prepare_data()
        self.calculate_tanimoto_distance()
        self.optimize_dbscan()
        self.map_to_full_dataset()
        print("This method shows ~97% noise ratio due to sampling constraints.\n")
        return self

# this method faced severe computational limitations
# With approximately 42,000 molecules and 1024-bit fingerprints
# constructing the full Tanimoto distance matrix required over 1.6 billion pairwise calculations
# consuming tens of gigabytes of memory and resulting in extremely low efficiency


# Molecular Descriptors + DBSCAN

class MolecularDescriptorClustering:
    def __init__(self, dataset):
        self.dataset = dataset
        self.X = None
        self.X_scaled = None
        self.X_pca = None
        self.best_eps = None
        self.labels = None

    def prepare_features(self):
        print("=" * 70)
        print("METHOD 2: Molecular Descriptors + DBSCAN (Recommended)")
        print("=" * 70)
        self.X = Molecular_Descriptors(self.dataset)
        X_features = self.X[["MolWt", "NumHDonors", "NumHAcceptors", "NumRotatableBonds"]]
        scaler = StandardScaler()
        self.X_scaled = scaler.fit_transform(X_features)
        print(f"Extracted {len(X_features.columns)} descriptors")
        print(f"Total molecules: {len(self.X)}\n")
        return self

    def perform_pca(self, variance_threshold=0.9):
        print("Performing PCA for dimensionality reduction...")
        pca_full = PCA().fit(self.X_scaled)
        cumvar = np.cumsum(pca_full.explained_variance_ratio_)
        n_components = np.argmax(cumvar >= variance_threshold) + 1
        self.X_pca = PCA(n_components=n_components, random_state=42).fit_transform(self.X_scaled)
        print(f"Selected {n_components} components (>{variance_threshold*100}% variance)\n")
        return self

    def optimize_dbscan(self, eps_values=[0.3, 0.4, 0.5, 0.6]):
        print("Optimizing DBSCAN eps parameter:")
        sil_scores = []
        for eps in eps_values:
            db = DBSCAN(eps=eps, min_samples=5)
            labels = db.fit_predict(self.X_pca)
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            sil = silhouette_score(self.X_pca, labels) if n_clusters > 1 else -1
            sil_scores.append(sil)
            print(f"  eps={eps:.2f}, clusters={n_clusters}, silhouette={sil:.3f}")
        self.best_eps = eps_values[np.argmax(sil_scores)]
        print(f"\nOptimal eps selected: {self.best_eps:.2f}\n")
        return self

    def cluster(self):
        print("Performing DBSCAN clustering...")
        db = DBSCAN(eps=self.best_eps, min_samples=5)
        self.labels = db.fit_predict(self.X_pca)
        self.X["Cluster"] = self.labels
        self.cluster_labels = self.labels
        unique, counts = np.unique(self.labels, return_counts=True)
        print("Cluster distribution:")
        print(pd.DataFrame({"Cluster": unique, "Count": counts}))
        return self

    def analyze_clusters(self):
        print("Analyzing cluster characteristics...")
        cluster_activity = self.X.groupby("Cluster")["HIV_active"].mean()
        desc_means = self.X.groupby("Cluster")[["MolWt", "NumHDonors", "NumHAcceptors", "NumRotatableBonds"]].mean()
        desc_means["HIV_active_ratio"] = cluster_activity
        print("\nAverage molecular descriptors per cluster:")
        print(desc_means)
        return cluster_activity, desc_means

    def visualize_clusters(self, cluster_activity, desc_means):
        plt.figure(figsize=(8, 5))
        cluster_activity.plot(kind="bar", color="teal")
        plt.title("Average HIV Active Ratio per Cluster")
        plt.xlabel("Cluster ID")
        plt.ylabel("HIV Active Ratio")
        plt.xticks(rotation=0)
        plt.tight_layout()
        plt.show()

        plt.figure(figsize=(10, 5))
        sns.heatmap(desc_means.T, annot=True, cmap="YlGnBu", fmt=".2f")
        plt.title("Average Molecular Descriptors per Cluster")
        plt.xlabel("Cluster ID")
        plt.ylabel("Descriptor")
        plt.tight_layout()
        plt.show()

    def evaluate_as_classifier(self): # suggested and revised by AI
        print("\n" + "=" * 70)
        print("Evaluating Clustering as Activity Predictor")
        print("=" * 70)
        mean_activity = self.X["HIV_active"].mean()
        cluster_activity = self.X.groupby("Cluster")["HIV_active"].mean()
        active_clusters = cluster_activity[cluster_activity > mean_activity].index.tolist()
        print(f"Overall activity rate: {mean_activity:.3f}")
        print(f"Active clusters: {active_clusters}\n")
        self.X["Predicted_active"] = self.X["Cluster"].apply(lambda c: 1 if c in active_clusters else 0)
        print("Classification Report:")
        print(classification_report(self.X["HIV_active"], self.X["Predicted_active"], digits=3))

    def analyze_high_activity_clusters(self, clusters_to_analyze=[1, 8]):
        import matplotlib.pyplot as plt
        from rdkit import Chem
        from rdkit.Chem import Descriptors, rdMolDescriptors, Draw
        from rdkit.Chem.Scaffolds import MurckoScaffold
        from collections import Counter
        from scipy.stats import ttest_ind

        print("\n" + "=" * 80)
        print(f"üîç Analyzing high-activity clusters: {clusters_to_analyze}")
        print("=" * 80)

        df = pd.DataFrame({
            "SMILES": self.dataset.canonical_smiles,
            "cluster": self.cluster_labels,
            "HIV_active": self.X["HIV_active"].values,
        })
        df_sub = df[df["cluster"].isin(clusters_to_analyze)].copy()
        df_sub["Mol"] = [Chem.MolFromSmiles(s) for s in df_sub["SMILES"] if Chem.MolFromSmiles(s)]

        # Murcko scaffold
        def mol_to_scaffold(m):
            try:
                return Chem.MolToSmiles(MurckoScaffold.GetScaffoldForMol(m))
            except:
                return None

        df_sub["Scaffold"] = df_sub["Mol"].apply(mol_to_scaffold)
        for c in clusters_to_analyze:
            scafs = df_sub.loc[df_sub["cluster"] == c, "Scaffold"].dropna().tolist()
            top_scaffolds = Counter(scafs).most_common(5)
            print(f"\nCluster {c} top scaffolds:")
            for smi, count in top_scaffolds:
                print(f"  {smi:<25} √ó{count}")
            mols = [Chem.MolFromSmiles(smi) for smi, _ in top_scaffolds if Chem.MolFromSmiles(smi)]
            if mols:
                img = Draw.MolsToGridImage(mols, molsPerRow=3, subImgSize=(250, 250))
                img.save(f"cluster_{c}_top_scaffolds.png")
                print(f"Saved: cluster_{c}_top_scaffolds.png")

        # test functional groups
        SMARTS = {
            "amide": "C(=O)N", "amine": "[NX3;H2,H1;!$(NC=O)]", "carboxy": "C(=O)[O;H,-]",
            "ester": "C(=O)O[A;!H0]", "ether": "C-O-C", "halide": "[F,Cl,Br,I]",
            "nitrile": "C#N", "nitro": "[$([NX3](=O)=O)]", "sulfonamide": "S(=O)(=O)N",
            "phenyl": "c1ccccc1", "alcohol": "[CX4][OX2H]", "phenol": "c1ccc(cc1)O",
        }
        compiled = {k: Chem.MolFromSmarts(v) for k, v in SMARTS.items()}

        def detect_groups(mol):
            return {k: int(mol.HasSubstructMatch(v)) for k, v in compiled.items()}

        fg_df = pd.DataFrame([{**detect_groups(m), "cluster": c}
                              for m, c in zip(df_sub["Mol"], df_sub["cluster"])]).dropna()
        fg_summary = fg_df.groupby("cluster").mean()
        print("\nFunctional group presence ratio per cluster:")
        print(fg_summary.loc[clusters_to_analyze])

        fg_summary.loc[clusters_to_analyze].T.plot(kind="bar", figsize=(12, 5))
        plt.title("Functional group presence ratio by cluster")
        plt.ylabel("Presence ratio")
        plt.tight_layout()
        plt.savefig("functional_groups_by_cluster.png")
        plt.close()
        print("Saved: functional_groups_by_cluster.png")

        # disect descriptors
        def get_desc(m):
            return pd.Series({
                "MolWt": Descriptors.MolWt(m),
                "LogP": Descriptors.MolLogP(m),
                "TPSA": rdMolDescriptors.CalcTPSA(m),
                "HBD": rdMolDescriptors.CalcNumHBD(m),
                "HBA": rdMolDescriptors.CalcNumHBA(m),
                "RB": rdMolDescriptors.CalcNumRotatableBonds(m),
            })

        desc_df = df_sub[["cluster", "Mol"]].copy()
        desc_df = pd.concat([desc_df, desc_df["Mol"].apply(get_desc)], axis=1)
        numeric_cols = ["MolWt", "LogP", "TPSA", "HBD", "HBA", "RB"]
        means = desc_df.groupby("cluster")[numeric_cols].mean()
        stds = desc_df.groupby("cluster")[numeric_cols].std()

        print("\nDescriptor comparison (mean ¬± std):")
        for d in numeric_cols:
            for c in clusters_to_analyze:
                if c in means.index:
                    print(f"{d:<5s} Cluster {c}: {means.loc[c,d]:.3f} ¬± {stds.loc[c,d]:.3f}")
        if len(clusters_to_analyze) == 2:
            c1, c2 = clusters_to_analyze
            for d in numeric_cols:
                v1 = desc_df[desc_df["cluster"] == c1][d].values
                v2 = desc_df[desc_df["cluster"] == c2][d].values
                if len(v1) > 1 and len(v2) > 1:
                    _, p = ttest_ind(v1, v2, equal_var=False)
                    print(f"  t-test {d}: p={p:.3e}")

    def run(self):
        self.prepare_features()
        self.perform_pca()
        self.optimize_dbscan()
        self.cluster()
        cluster_activity, desc_means = self.analyze_clusters()
        self.visualize_clusters(cluster_activity, desc_means)
        self.evaluate_as_classifier()
        print("\nMolecular descriptor clustering completed successfully!\n")
        return self


if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("Running Method 2: Molecular Descriptors + DBSCAN")
    print("=" * 70 + "\n")
    clustering = MolecularDescriptorClustering(dataset)
    clustering.run()
    clustering.analyze_high_activity_clusters([1, 8])
